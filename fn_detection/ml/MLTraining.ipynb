{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from config import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import shap\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenewsnet = pd.read_csv('../data/fakenewsnet_wf.csv')\n",
    "isot = pd.read_csv('../data/fn_isot_wf.csv')\n",
    "fakenewskaggle = pd.read_csv('../data/fn_kaggle_wf.csv')\n",
    "buzfeed_political = pd.read_csv('../data/fn_buzfeed_wf.csv')\n",
    "celebrity = pd.read_csv('../data/fn_celebrity_wf.csv')\n",
    "fakenewsamt = pd.read_csv('../data/fakenewsamt_wf.csv')\n",
    "fn_randompolitical = pd.read_csv('../data/fn_randompolitical_wf.csv')\n",
    "\n",
    "datasets = {\n",
    "    'FakeNewsNet' : fakenewsnet,\n",
    "    'ISOT' : isot,\n",
    "    'FakeNewsKaggle' : fakenewskaggle,\n",
    "    'FakeNewsAMT' : fakenewsamt,\n",
    "    'FakeNewsRandomPolitical' : fn_randompolitical,\n",
    "    'FakeNewsCelebrity' : celebrity,\n",
    "    'FakeNewsBuzfeedPolitical' : buzfeed_political,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    'Moral' : MORAL_FEATURES,\n",
    "    'ReadabilityGrades' : READABILITY_GRADE_FEATURES,\n",
    "    'ReadabilitySentenceInfo' : READABILITY_SENTENCEINFO_FEATURES,\n",
    "    'ReadabilitySentenceBegininng' : READABILITY_SENTENCEBEGINNING_FEATURES,\n",
    "    'ReadabilityWordUsage' : READABILITY_WORDUSAGE_FEATURES,\n",
    "    'AllReadability' : READABILITY_GRADE_FEATURES + READABILITY_SENTENCEINFO_FEATURES + READABILITY_SENTENCEBEGINNING_FEATURES + READABILITY_WORDUSAGE_FEATURES,\n",
    "    'Sentiment' : SENTIMENT_FEATURES,\n",
    "    'LIWCLinguistic' : LIWC_LINGUISTIC_FEATURES,\n",
    "    'LIWCAffectiveProcesses' : LIWC_AFFECTIVEPROCESSES_FEATURES,\n",
    "    'LIWCSocialProcesses' : LIWC_SOCIALPROCESSES_FEATURES,\n",
    "    'LIWCCognitiveProcesses' : LIWC_COGNITIVEPROCESSES_FEATURES,\n",
    "    'LIWCPerceptualProcesses' : LIWC_PERCEPTUALPROCESSES_FEATURES,\n",
    "    'LIWCBiologicalProcesses' : LIWC_BIOLOGICALPROCESSES_FEATURES,\n",
    "    'LIWCDrives' : LIWC_DRIVES_FEATURES,\n",
    "    'LIWCTimeOrientation' : LIWC_TIMEORIENTATION_FEATURES,\n",
    "    'LIWCRelativity' : LIWC_RELATIVITY_FEATURES,\n",
    "    'LIWCPersonalConcerns' : LIWC_PERSONALCONCERNS_FEATURES,\n",
    "    'LIWCInformalLanguage' : LIWC_INFORMALLANGUAGE_FEATURES,\n",
    "    'AllLIWC' : LIWC_LINGUISTIC_FEATURES + LIWC_AFFECTIVEPROCESSES_FEATURES + LIWC_SOCIALPROCESSES_FEATURES + LIWC_COGNITIVEPROCESSES_FEATURES + LIWC_PERCEPTUALPROCESSES_FEATURES + LIWC_BIOLOGICALPROCESSES_FEATURES \n",
    "        + LIWC_DRIVES_FEATURES + LIWC_TIMEORIENTATION_FEATURES + LIWC_RELATIVITY_FEATURES + LIWC_PERSONALCONCERNS_FEATURES + LIWC_INFORMALLANGUAGE_FEATURES,\n",
    "    'All' : MORAL_FEATURES + READABILITY_GRADE_FEATURES + READABILITY_SENTENCEINFO_FEATURES + READABILITY_SENTENCEBEGINNING_FEATURES + READABILITY_WORDUSAGE_FEATURES + SENTIMENT_FEATURES + LIWC_LINGUISTIC_FEATURES + LIWC_AFFECTIVEPROCESSES_FEATURES + LIWC_SOCIALPROCESSES_FEATURES + LIWC_COGNITIVEPROCESSES_FEATURES + LIWC_PERCEPTUALPROCESSES_FEATURES + LIWC_BIOLOGICALPROCESSES_FEATURES \n",
    "        + LIWC_DRIVES_FEATURES + LIWC_TIMEORIENTATION_FEATURES + LIWC_RELATIVITY_FEATURES + LIWC_PERSONALCONCERNS_FEATURES + LIWC_INFORMALLANGUAGE_FEATURES\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df):   \n",
    "    start_memory = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"Memory usage of dataframe is {start_memory} MB\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != 'object':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "                    \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    end_memory = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n",
    "    print(f\"Reduced by {100 * (start_memory - end_memory) / start_memory} % \")\n",
    "    return df\n",
    "\n",
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model \n",
    "        to generalize and improves the interpretability of the model.\n",
    "\n",
    "    Inputs: \n",
    "        x: features dataframe\n",
    "        threshold: features with correlations greater than this value are removed\n",
    "\n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i+1):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "\n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                #print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    # x = x.drop(columns=drops)\n",
    "    return drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Dataset FakeNewsNet---\n",
      "Memory usage of dataframe is 0.08171844482421875 MB\n",
      "Memory usage of dataframe after reduction 0.08171844482421875 MB\n",
      "Reduced by 0.0 % \n",
      "---Dataset ISOT---\n",
      "Memory usage of dataframe is 9.591863632202148 MB\n",
      "Memory usage of dataframe after reduction 9.591863632202148 MB\n",
      "Reduced by 0.0 % \n",
      "---Dataset FakeNewsKaggle---\n",
      "Memory usage of dataframe is 3.9293441772460938 MB\n",
      "Memory usage of dataframe after reduction 3.9293441772460938 MB\n",
      "Reduced by 0.0 % \n",
      "---Dataset FakeNewsAMT---\n",
      "Memory usage of dataframe is 0.10540771484375 MB\n",
      "Memory usage of dataframe after reduction 0.10540771484375 MB\n",
      "Reduced by 0.0 % \n",
      "---Dataset FakeNewsRandomPolitical---\n",
      "Memory usage of dataframe is 0.033023834228515625 MB\n",
      "Memory usage of dataframe after reduction 0.033023834228515625 MB\n",
      "Reduced by 0.0 % \n",
      "---Dataset FakeNewsCelebrity---\n",
      "Memory usage of dataframe is 0.10979461669921875 MB\n",
      "Memory usage of dataframe after reduction 0.10979461669921875 MB\n",
      "Reduced by 0.0 % \n",
      "---Dataset FakeNewsBuzfeedPolitical---\n",
      "Memory usage of dataframe is 0.022275924682617188 MB\n",
      "Memory usage of dataframe after reduction 0.022275924682617188 MB\n",
      "Reduced by 0.0 % \n",
      "Removing 9 collinear features\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = []\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print('---Dataset {dataset_name}---'.format(dataset_name=dataset_name))\n",
    "    df = dataset[feature_sets['All']]\n",
    "    df = df.loc[:,~df.columns.duplicated()].copy()\n",
    "    datasets[dataset_name] = reduce_memory_usage(df)\n",
    "    cols_to_drop.append(remove_collinear_features(datasets[dataset_name] ,0.95))\n",
    "\n",
    "cols_to_drop = set.intersection(*cols_to_drop)\n",
    "print('Removing {cols_to_drop} collinear features'.format(cols_to_drop=len(cols_to_drop)))\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    datasets[dataset_name] = datasets[dataset_name].drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\n",
    "    'DecisionTree',\n",
    "    'SVC' ,\n",
    "    'LogisticRegression',\n",
    "    'RandomForest',\n",
    "    'XGBoost',\n",
    "    'CatBoost',\n",
    "]\n",
    "\n",
    "def get_algorithm(name):\n",
    "    if name == 'XGBoost' : \n",
    "        return XGBClassifier(n_jobs=-1)\n",
    "    elif name == 'CatBoost' :\n",
    "        return CatBoostClassifier(verbose=False)\n",
    "    elif name == 'DecisionTree' :\n",
    "        return DecisionTreeClassifier(class_weight='balanced')\n",
    "    elif name == 'SVC' :\n",
    "        return SVC(class_weight='balanced')\n",
    "    elif name == 'LinearSVC':\n",
    "        return LinearSVC(class_weight='balanced')\n",
    "    elif name == 'RandomForest' :\n",
    "        return RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n",
    "    elif name == 'LogisticRegression' :\n",
    "        return LogisticRegression(class_weight='balanced', n_jobs=-1, max_iter=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Dataset FakeNewsNet---\n",
      "---Algorithm DecisionTree---\n",
      "0.6557379434687944\n",
      "\n",
      "---Algorithm SVC---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33165/4195899973.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71991547475758\n",
      "\n",
      "---Algorithm LogisticRegression---\n",
      "0.7007210501388558\n",
      "\n",
      "---Algorithm RandomForest---\n",
      "0.7074419556214377\n",
      "\n",
      "---Algorithm XGBoost---\n",
      "0.7275630067839993\n",
      "\n",
      "---Algorithm CatBoost---\n",
      "0.7319970390007949\n",
      "\n",
      "---Dataset ISOT---\n",
      "---Algorithm DecisionTree---\n",
      "0.9370923483977449\n",
      "\n",
      "---Algorithm SVC---\n",
      "0.9737012919016859\n",
      "\n",
      "---Algorithm LogisticRegression---\n",
      "0.9617416918716714\n",
      "\n",
      "---Algorithm RandomForest---\n",
      "0.9702461457904057\n",
      "\n",
      "---Algorithm XGBoost---\n",
      "0.9776094813986049\n",
      "\n",
      "---Algorithm CatBoost---\n",
      "0.9787764713349894\n",
      "\n",
      "---Dataset FakeNewsKaggle---\n",
      "---Algorithm DecisionTree---\n",
      "0.7694419787641034\n",
      "\n",
      "---Algorithm SVC---\n",
      "0.8761908651061081\n",
      "\n",
      "---Algorithm LogisticRegression---\n",
      "0.8446943142089631\n",
      "\n",
      "---Algorithm RandomForest---\n",
      "0.853593897189511\n",
      "\n",
      "---Algorithm XGBoost---\n",
      "0.8824279758461856\n",
      "\n",
      "---Algorithm CatBoost---\n",
      "0.8820630384813628\n",
      "\n",
      "---Dataset FakeNewsAMT---\n",
      "---Algorithm DecisionTree---\n",
      "0.5422862586096804\n",
      "\n",
      "---Algorithm SVC---\n",
      "0.6090561410278544\n",
      "\n",
      "---Algorithm LogisticRegression---\n",
      "0.6762662561896093\n",
      "\n",
      "---Algorithm RandomForest---\n",
      "0.6230176583964242\n",
      "\n",
      "---Algorithm XGBoost---\n",
      "0.6215760212667463\n",
      "\n",
      "---Algorithm CatBoost---\n",
      "0.6654989316503606\n",
      "\n",
      "---Dataset FakeNewsRandomPolitical---\n",
      "---Algorithm DecisionTree---\n",
      "0.711658120636806\n",
      "\n",
      "---Algorithm SVC---\n",
      "0.7779016565126948\n",
      "\n",
      "---Algorithm LogisticRegression---\n",
      "0.7926266618994651\n",
      "\n",
      "---Algorithm RandomForest---\n",
      "0.7780399964958788\n",
      "\n",
      "---Algorithm XGBoost---\n",
      "0.771024329047132\n",
      "\n",
      "---Algorithm CatBoost---\n",
      "0.8061387652947719\n",
      "\n",
      "---Dataset FakeNewsCelebrity---\n",
      "---Algorithm DecisionTree---\n",
      "0.6631688690603623\n",
      "\n",
      "---Algorithm SVC---\n",
      "0.7433628663455406\n",
      "\n",
      "---Algorithm LogisticRegression---\n",
      "0.7017162825371658\n",
      "\n",
      "---Algorithm RandomForest---\n",
      "0.7497007845712107\n",
      "\n",
      "---Algorithm XGBoost---\n",
      "0.7431265379395514\n",
      "\n",
      "---Algorithm CatBoost---\n",
      "0.7733268634369159\n",
      "\n",
      "---Dataset FakeNewsBuzfeedPolitical---\n",
      "---Algorithm DecisionTree---\n",
      "0.7485560782526727\n",
      "\n",
      "---Algorithm SVC---\n",
      "0.804884056002477\n",
      "\n",
      "---Algorithm LogisticRegression---\n",
      "0.7979364267446108\n",
      "\n",
      "---Algorithm RandomForest---\n",
      "0.8176356748649116\n",
      "\n",
      "---Algorithm XGBoost---\n",
      "0.7785779262972244\n",
      "\n",
      "---Algorithm CatBoost---\n",
      "0.8291089756476755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(240993)\n",
    "\n",
    "# create dataframe for results\n",
    "results_df = pd.DataFrame(columns=['dataset', 'algorithm', 'f1_weighted_mean', 'f1_weighted_std'])\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print('---Dataset {dataset_name}---'.format(dataset_name=dataset_name))\n",
    "\n",
    "    X = dataset[feature_sets['All']]\n",
    "    y = dataset['label']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    for algorithm_name in algorithms:\n",
    "        print('---Algorithm {algorithms_name}---'.format(algorithms_name=algorithm_name))\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=24091993)\n",
    "        scores = cross_val_score(get_algorithm(algorithm_name), X, y, cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
    "        print(np.mean(scores))\n",
    "        print()\n",
    "\n",
    "        # add results to dataframe using concat method\n",
    "        results_df = pd.concat([results_df, pd.DataFrame({\n",
    "            'dataset' : [dataset_name],\n",
    "            'algorithm' : [algorithm_name],\n",
    "            'f1_weighted_mean' : np.mean(scores),\n",
    "            'f1_weighted_std' : np.std(scores)\n",
    "        })], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>f1_weighted_mean</th>\n",
       "      <th>f1_weighted_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.043068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.719915</td>\n",
       "      <td>0.038878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.700721</td>\n",
       "      <td>0.042410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.707442</td>\n",
       "      <td>0.027259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.727563</td>\n",
       "      <td>0.016008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.731997</td>\n",
       "      <td>0.032924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.937092</td>\n",
       "      <td>0.001938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.973701</td>\n",
       "      <td>0.001852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.961742</td>\n",
       "      <td>0.001283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.970246</td>\n",
       "      <td>0.000964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.977609</td>\n",
       "      <td>0.001074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ISOT</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.978776</td>\n",
       "      <td>0.000696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FakeNewsKaggle</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.769442</td>\n",
       "      <td>0.007076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FakeNewsKaggle</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.876191</td>\n",
       "      <td>0.004005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FakeNewsKaggle</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.844694</td>\n",
       "      <td>0.006493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FakeNewsKaggle</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.853594</td>\n",
       "      <td>0.005447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FakeNewsKaggle</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.882428</td>\n",
       "      <td>0.001152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FakeNewsKaggle</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.882063</td>\n",
       "      <td>0.004432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FakeNewsAMT</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.542286</td>\n",
       "      <td>0.029343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FakeNewsAMT</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.609056</td>\n",
       "      <td>0.025478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FakeNewsAMT</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.676266</td>\n",
       "      <td>0.019493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FakeNewsAMT</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.623018</td>\n",
       "      <td>0.033737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FakeNewsAMT</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.621576</td>\n",
       "      <td>0.035316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FakeNewsAMT</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.665499</td>\n",
       "      <td>0.047503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>FakeNewsRandomPolitical</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.711658</td>\n",
       "      <td>0.072299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>FakeNewsRandomPolitical</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.777902</td>\n",
       "      <td>0.050559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>FakeNewsRandomPolitical</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.792627</td>\n",
       "      <td>0.074625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>FakeNewsRandomPolitical</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.778040</td>\n",
       "      <td>0.073571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>FakeNewsRandomPolitical</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.771024</td>\n",
       "      <td>0.063901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>FakeNewsRandomPolitical</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.806139</td>\n",
       "      <td>0.083137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>FakeNewsCelebrity</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.663169</td>\n",
       "      <td>0.027787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>FakeNewsCelebrity</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.033623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>FakeNewsCelebrity</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.701716</td>\n",
       "      <td>0.044602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>FakeNewsCelebrity</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.749701</td>\n",
       "      <td>0.025764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>FakeNewsCelebrity</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.743127</td>\n",
       "      <td>0.023261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>FakeNewsCelebrity</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.773327</td>\n",
       "      <td>0.034862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>FakeNewsBuzfeedPolitical</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.748556</td>\n",
       "      <td>0.072221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>FakeNewsBuzfeedPolitical</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.804884</td>\n",
       "      <td>0.107231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>FakeNewsBuzfeedPolitical</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.797936</td>\n",
       "      <td>0.088891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>FakeNewsBuzfeedPolitical</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.817636</td>\n",
       "      <td>0.092547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>FakeNewsBuzfeedPolitical</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.778578</td>\n",
       "      <td>0.087411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>FakeNewsBuzfeedPolitical</td>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.829109</td>\n",
       "      <td>0.070262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     dataset           algorithm  f1_weighted_mean  \\\n",
       "0                FakeNewsNet        DecisionTree          0.655738   \n",
       "1                FakeNewsNet                 SVC          0.719915   \n",
       "2                FakeNewsNet  LogisticRegression          0.700721   \n",
       "3                FakeNewsNet        RandomForest          0.707442   \n",
       "4                FakeNewsNet             XGBoost          0.727563   \n",
       "5                FakeNewsNet            CatBoost          0.731997   \n",
       "6                       ISOT        DecisionTree          0.937092   \n",
       "7                       ISOT                 SVC          0.973701   \n",
       "8                       ISOT  LogisticRegression          0.961742   \n",
       "9                       ISOT        RandomForest          0.970246   \n",
       "10                      ISOT             XGBoost          0.977609   \n",
       "11                      ISOT            CatBoost          0.978776   \n",
       "12            FakeNewsKaggle        DecisionTree          0.769442   \n",
       "13            FakeNewsKaggle                 SVC          0.876191   \n",
       "14            FakeNewsKaggle  LogisticRegression          0.844694   \n",
       "15            FakeNewsKaggle        RandomForest          0.853594   \n",
       "16            FakeNewsKaggle             XGBoost          0.882428   \n",
       "17            FakeNewsKaggle            CatBoost          0.882063   \n",
       "18               FakeNewsAMT        DecisionTree          0.542286   \n",
       "19               FakeNewsAMT                 SVC          0.609056   \n",
       "20               FakeNewsAMT  LogisticRegression          0.676266   \n",
       "21               FakeNewsAMT        RandomForest          0.623018   \n",
       "22               FakeNewsAMT             XGBoost          0.621576   \n",
       "23               FakeNewsAMT            CatBoost          0.665499   \n",
       "24   FakeNewsRandomPolitical        DecisionTree          0.711658   \n",
       "25   FakeNewsRandomPolitical                 SVC          0.777902   \n",
       "26   FakeNewsRandomPolitical  LogisticRegression          0.792627   \n",
       "27   FakeNewsRandomPolitical        RandomForest          0.778040   \n",
       "28   FakeNewsRandomPolitical             XGBoost          0.771024   \n",
       "29   FakeNewsRandomPolitical            CatBoost          0.806139   \n",
       "30         FakeNewsCelebrity        DecisionTree          0.663169   \n",
       "31         FakeNewsCelebrity                 SVC          0.743363   \n",
       "32         FakeNewsCelebrity  LogisticRegression          0.701716   \n",
       "33         FakeNewsCelebrity        RandomForest          0.749701   \n",
       "34         FakeNewsCelebrity             XGBoost          0.743127   \n",
       "35         FakeNewsCelebrity            CatBoost          0.773327   \n",
       "36  FakeNewsBuzfeedPolitical        DecisionTree          0.748556   \n",
       "37  FakeNewsBuzfeedPolitical                 SVC          0.804884   \n",
       "38  FakeNewsBuzfeedPolitical  LogisticRegression          0.797936   \n",
       "39  FakeNewsBuzfeedPolitical        RandomForest          0.817636   \n",
       "40  FakeNewsBuzfeedPolitical             XGBoost          0.778578   \n",
       "41  FakeNewsBuzfeedPolitical            CatBoost          0.829109   \n",
       "\n",
       "    f1_weighted_std  \n",
       "0          0.043068  \n",
       "1          0.038878  \n",
       "2          0.042410  \n",
       "3          0.027259  \n",
       "4          0.016008  \n",
       "5          0.032924  \n",
       "6          0.001938  \n",
       "7          0.001852  \n",
       "8          0.001283  \n",
       "9          0.000964  \n",
       "10         0.001074  \n",
       "11         0.000696  \n",
       "12         0.007076  \n",
       "13         0.004005  \n",
       "14         0.006493  \n",
       "15         0.005447  \n",
       "16         0.001152  \n",
       "17         0.004432  \n",
       "18         0.029343  \n",
       "19         0.025478  \n",
       "20         0.019493  \n",
       "21         0.033737  \n",
       "22         0.035316  \n",
       "23         0.047503  \n",
       "24         0.072299  \n",
       "25         0.050559  \n",
       "26         0.074625  \n",
       "27         0.073571  \n",
       "28         0.063901  \n",
       "29         0.083137  \n",
       "30         0.027787  \n",
       "31         0.033623  \n",
       "32         0.044602  \n",
       "33         0.025764  \n",
       "34         0.023261  \n",
       "35         0.034862  \n",
       "36         0.072221  \n",
       "37         0.107231  \n",
       "38         0.088891  \n",
       "39         0.092547  \n",
       "40         0.087411  \n",
       "41         0.070262  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_weighted_mean</th>\n",
       "      <th>f1_weighted_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.809559</td>\n",
       "      <td>0.039117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.718277</td>\n",
       "      <td>0.036247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.782243</td>\n",
       "      <td>0.039685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.785668</td>\n",
       "      <td>0.037041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.786430</td>\n",
       "      <td>0.037375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.785986</td>\n",
       "      <td>0.032589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    f1_weighted_mean  f1_weighted_std\n",
       "algorithm                                            \n",
       "CatBoost                    0.809559         0.039117\n",
       "DecisionTree                0.718277         0.036247\n",
       "LogisticRegression          0.782243         0.039685\n",
       "RandomForest                0.785668         0.037041\n",
       "SVC                         0.786430         0.037375\n",
       "XGBoost                     0.785986         0.032589"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.groupby('algorithm').mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(240993)\n",
    "\n",
    "\n",
    "algorithm_name = 'SVC'\n",
    "\n",
    "for datasets_name, dataset in datasets.items():\n",
    "    print('---Dataset {datasets_name}---'.format(datasets_name=datasets_name))\n",
    "\n",
    "    X = dataset[feature_sets['All']]\n",
    "    y = dataset['label']\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=24091993)\n",
    "\n",
    "    # create model\n",
    "\n",
    "    model = get_algorithm(algorithm_name)\n",
    "\n",
    "    # create parameter grid\n",
    "    param_grid = {\n",
    "        'C' : [0.001, 0.01, 0.1, 1, 10],\n",
    "        'gamma' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    }\n",
    "\n",
    "    # create grid search object\n",
    "    grid_search = GridSearchCV(model, param_grid=param_grid, cv=cv, scoring='f1_weighted', n_jobs=-1)\n",
    "\n",
    "    # fit grid search object\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # print best parameters\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # print best score\n",
    "    print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Dataset FakeNewsNet---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.63      0.72        43\n",
      "           1       0.63      0.84      0.72        32\n",
      "\n",
      "    accuracy                           0.72        75\n",
      "   macro avg       0.74      0.74      0.72        75\n",
      "weighted avg       0.75      0.72      0.72        75\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 373it [08:34,  1.41s/it]                         \n",
      "/tmp/ipykernel_17630/380262918.py:48: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_shap = pd.concat([results_shap, shap_importance], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Dataset ISOT---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      4289\n",
      "           1       0.97      0.98      0.97      4457\n",
      "\n",
      "    accuracy                           0.97      8746\n",
      "   macro avg       0.97      0.97      0.97      8746\n",
      "weighted avg       0.97      0.97      0.97      8746\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer:   0%|          | 9/43729 [04:42<428:34:28, 35.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(model\u001b[38;5;241m.\u001b[39mpredict, background_X)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# create shap values\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m feature_sets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     40\u001b[0m vals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(shap_values\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/agents-as-mediators/lib/python3.10/site-packages/shap/explainers/_permutation.py:77\u001b[0m, in \u001b[0;36mPermutationExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, main_effects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error_bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m              outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/agents-as-mediators/lib/python3.10/site-packages/shap/explainers/_explainer.py:266\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), num_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent):\n\u001b[0;32m--> 266\u001b[0m     row_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    271\u001b[0m     output_indices\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/.virtualenvs/agents-as-mediators/lib/python3.10/site-packages/shap/explainers/_permutation.py:133\u001b[0m, in \u001b[0;36mPermutationExplainer.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[1;32m    130\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# evaluate the masked model\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     row_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(fm),) \u001b[38;5;241m+\u001b[39m outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m~/.virtualenvs/agents-as-mediators/lib/python3.10/site-packages/shap/utils/_masked_model.py:60\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(masks\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupports_delta_masking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delta_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# we need to convert from delta masking to a full masking call because we were given a delta masking\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# input but the masker does not support delta masking\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         full_masks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(masks \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masker_cols), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/agents-as-mediators/lib/python3.10/site-packages/shap/utils/_masked_model.py:206\u001b[0m, in \u001b[0;36mMaskedModel._delta_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    203\u001b[0m     batch_positions[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_positions[i] \u001b[38;5;241m+\u001b[39m num_varying_rows[i]\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# joined_masked_inputs = self._stack_inputs(all_masked_inputs)\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msubset_masked_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m _assert_output_input_match(subset_masked_inputs, outputs)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinearize_link \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlink \u001b[38;5;241m!=\u001b[39m links\u001b[38;5;241m.\u001b[39midentity \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linearizing_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/agents-as-mediators/lib/python3.10/site-packages/shap/models/_model.py:21\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     is_tensor \u001b[38;5;241m=\u001b[39m safe_isinstance(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m is_tensor \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(out)\n",
      "File \u001b[0;32m~/.virtualenvs/agents-as-mediators/lib/python3.10/site-packages/sklearn/svm/_base.py:813\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    811\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 813\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[0;32m~/.virtualenvs/agents-as-mediators/lib/python3.10/site-packages/sklearn/svm/_base.py:430\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    428\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_for_predict(X)\n\u001b[1;32m    429\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/agents-as-mediators/lib/python3.10/site-packages/sklearn/svm/_base.py:449\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    442\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be equal to \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe number of samples at training time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    445\u001b[0m         )\n\u001b[1;32m    447\u001b[0m svm_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_vectors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_support\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dual_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msvm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create dataframe for results\n",
    "results_shap = pd.DataFrame(columns=['dataset', 'col_name','feature_importance_vals'])\n",
    "\n",
    "np.random.seed(240993)\n",
    "\n",
    "algorithm_name = 'SVC'\n",
    "\n",
    "for datasets_name, dataset in datasets.items():\n",
    "    print('---Dataset {datasets_name}---'.format(datasets_name=datasets_name))\n",
    "\n",
    "    X = dataset[feature_sets['All']]\n",
    "    y = dataset['label']\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # create model\n",
    "\n",
    "    model = get_algorithm(algorithm_name)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2409199)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # print classification report\n",
    "    print(classification_report(y_test, model.predict(X_test)))\n",
    "\n",
    "    model = get_algorithm(algorithm_name).fit(X, y)\n",
    "    \n",
    "    background_X = shap.maskers.Independent(X, max_samples=100)\n",
    "\n",
    "    # create explainer\n",
    "    explainer = shap.Explainer(model.predict, background_X)\n",
    "\n",
    "    # create shap values\n",
    "    shap_values = explainer(X)\n",
    "\n",
    "    feature_names = feature_sets['All']\n",
    "\n",
    "\n",
    "    vals = np.abs(shap_values.values).mean(0)\n",
    "\n",
    "    shap_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                  columns=['col_name','feature_importance_vals'])\n",
    "    shap_importance['dataset'] = datasets_name\n",
    "    shap_importance.sort_values(by=['feature_importance_vals'],\n",
    "                               ascending=False, inplace=True)\n",
    "    shap_importance.head()\n",
    "    results_shap = pd.concat([results_shap, shap_importance], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>col_name</th>\n",
       "      <th>feature_importance_vals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>liwc_hear</td>\n",
       "      <td>0.062265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>liwc_tentat</td>\n",
       "      <td>0.041552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>liwc_function</td>\n",
       "      <td>0.039899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>liwc_percept</td>\n",
       "      <td>0.036714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>liwc_they</td>\n",
       "      <td>0.034919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>readability_subordination</td>\n",
       "      <td>0.001815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>readability_pronoun</td>\n",
       "      <td>0.001707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>readability_tobeverb</td>\n",
       "      <td>0.001344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>readability_paragraphs</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>FakeNewsNet</td>\n",
       "      <td>readability_directspeech_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset                        col_name  feature_importance_vals\n",
       "0    FakeNewsNet                       liwc_hear                 0.062265\n",
       "1    FakeNewsNet                     liwc_tentat                 0.041552\n",
       "2    FakeNewsNet                   liwc_function                 0.039899\n",
       "3    FakeNewsNet                    liwc_percept                 0.036714\n",
       "4    FakeNewsNet                       liwc_they                 0.034919\n",
       "..           ...                             ...                      ...\n",
       "113  FakeNewsNet       readability_subordination                 0.001815\n",
       "114  FakeNewsNet             readability_pronoun                 0.001707\n",
       "115  FakeNewsNet            readability_tobeverb                 0.001344\n",
       "116  FakeNewsNet          readability_paragraphs                 0.000000\n",
       "117  FakeNewsNet  readability_directspeech_ratio                 0.000000\n",
       "\n",
       "[118 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17630/50124802.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/agents-as-mediators/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    882\u001b[0m                 )\n\u001b[1;32m    883\u001b[0m         \u001b[0;31m# For data is scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataFrame constructor not properly called!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(shap_values, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-as-mediators",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
